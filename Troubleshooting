CrashLoopBackOff:-

  *)This issue indicates a pod cannot be scheduled on a node. This could happen because the node does not have sufficient resources
    to run the pod, (or) because the pod did not succeed in mounting the requested volumes.
  *) to identify the issue
       $ kubectl get pods
  *) Check the output to see if the pod status is CrashLoopBackOff
       $ kubectl get pods
       NAME       READY    STATUS             RESTARTS   AGE
       mypod-1    0/1      CrashLoopBackOff   0          58s
      Getting detailed information and resolving the issue
  *)  Here are the common causes:-
     1)Insufficient resources:—if there are insufficient resources on the node, you can manually evict pods from the node (or) scale up your 
                               cluster to ensure more nodes are available for your pods.
     2) Volume mounting:—if you see the issue is mounting a storage volume, check which volume the pod is trying to mount, ensure it is defined 
                         correctly in the pod manifest, and see that a storage volume with those definitions is available.
     3) Use of hostPort:—if you are binding pods to a hostPort, you may only be able to schedule one pod per node. In most cases you can avoid
                         using hostPort and use a Service object to enable communication with your pod.

Kubernetes Node Not Ready:-

  *) When a worker node shuts down (or) crashes, all stateful pods that reside on it become unavailable, and the node status appears as NotReady.
  *) If a node has a NotReady status for over five minutes (by default), Kubernetes changes the status of pods scheduled on it to Unknown, and
     attempts to schedule it on another node, with status ContainerCreating.
  *)  to identify the issue
        $ kubectl get nodes.
  *) Check the output to see is the node status is NotReady
        NAME        STATUS      AGE    VERSION
        mynode-1    NotReady    1h     v1.2.0
  *) To check if pods scheduled on your node are being moved to other nodes, run the command get pods.
  *) Check the output to see if a pod appears twice on two different nodes, as follows
        NAME       READY    STATUS               RESTARTS      AGE    IP        NODE
        mypod-1    1/1      Unknown              0             10m    [IP]      mynode-1
        mypod-1    0/1      ContainerCreating    0             15s    [none]    mynode-2
  *) to Resolve this issue
     1) If the failed node is able to recover (or) is rebooted by the user, the issue will resolve itself. Once the failed node recovers and joins the
        cluster, the following process takes place
     2) The pod with Unknown status is deleted, and volumes are detached from the failed node.
     3) The pod is rescheduled on the new node, it's status changes from Unknown to ContainerCreating and required volumes are attached.
     4) Kubernetes uses a "5-minute" timeout (by default), after which the pod will run on the node, and its status changes from ContainerCreating 
        to Running.
     5) If you have no time to wait, (or) the node does not recover, you’ll need to help Kubernetes reschedule the stateful pods on another, working node.
        There are 2 ways to achieve this
          1) Remove failed node from the cluster—using the command $ kubectl delete node [name]
                 ex:- $ kubect delete node gk
          2) Delete stateful pods with status unknown—using the command $ kubectl delete pods [pod_name] --grace-period=0 --force -n [namespace]
                 ex:- $ kubectl delete pods gk --grace-period=0 --force -n prod

Pod Stays Pending:-

it means that the pod is not able to be scheduled onto a node

 1) Check Node Availability:-

    Ensure that your nodes are in the Ready state.
      $ kubectl get nodes

  2) Check Resource Constraints:-

      Inspect the resource requests and limits in your pod's YAML file. Ensure they are reasonable and can be satisfied by the nodes.
               resources:
                   requests:
                       memory: "64Mi"
                       cpu: "250m"
                   limits:
                        memory: "128Mi"
                        cpu: "500m"

 3) Check Node Resources:-

    Confirm that the node has enough resources to meet the pod's requirements.
          $ kubectl describe node <node-name>
      ex:-$ kubectl describe node gk

 4) Check Pod Events:-

    View events associated with the pod to identify any issues.
          $ kubectl describe pod <pod-name>
      ex:-$ kubectl describe pod gk
          $ kubectl top pods

 5) Check Network Policies:-

    Ensure that there are no network policies preventing the pod from being scheduled.
         $ kubectl get networkpolicies

 6) Check Persistent Volume (PV) and Persistent Volume Claim (PVC):-

     If your pod requires persistent storage, ensure that the PVC is bound to a PV.
         $ kubectl get pv
         $ kubectl get pvc

 7) Check for taints and tolerations:-

    Taints on nodes or tolerations in the pod specification may prevent scheduling.
         $ kubectl describe node <node-name> | grep Taints
     ex:-$ kubectl describe node gk | grep Taints
         $ kubectl get pods <pod-name> -o yaml | grep tolerations
     ex:-$ kubectl get pods gk -o yaml | grep tolerations

 8) Check Node Affinity:-

    Verify if node affinity rules in the pod specification are causing scheduling issues.
       affinity:
         nodeAffinity:
           requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
               - matchExpressions:
                 - key: <node-label-key>
                   operator: In
                   values:
                     - <node-label-value>

 9) Check for Resource Quotas and Limits:-

     Verify if resource quotas are applied and if they might be preventing pod scheduling.
         $ kubectl get resourcequotas

 10) Check Cluster Events:-

    Inspect the cluster events for any issues that might be affecting pod scheduling.
        $ kubectl get events --sort-by=.metadata.creationTimestamp

 11) Check Pod Configuration:-

     Ensure there are no syntax errors or misconfigurations in your pod's YAML file.
        $ kubectl apply -f <pod-definition-file.yaml> --dry-run=client

 12) Check for Image Pull Issues:-

     Verify that the container image specified in the pod definition exists and can be pulled.
          containers:
          - name: my-container
            image: my-image:tag
     ex:- - name: nginx-container
            image: nginx

Pod Stays Waiting:-

*) If a pod’s status is Waiting, this means it is scheduled on a node, but unable to run. Look at the describe pod output, in
   the ‘Events’ section, and try to identify reasons the pod is not able to run.
*) Most often, this will be due to an error when fetching the image. If so, check for the following
    1) Image name:— ensure the image name in the pod manifest is correct
    2) Image available:— ensure the image is really available in the repository
    3) Test manually:— run a docker pull command on the local machine, ensuring you have the appropriate permissions, to see if you
                       can retrieve the image.

Pod Is Running but Misbehaving:-

  it means that the containers within the pod have started successfully, but there are issues with the application (or) its behavior.

*) Check Pod Logs:-

   View the logs of the misbehaving container to identify any error messages (or) issues.
        $ kubectl logs <pod-name>
    ex:-$ kubectl logs gk

 *) Inspect Container State:-

    Use kubectl exec to run commands inside the container and inspect its state.
        $ kubectl exec -it <pod-name> -- /bin/bash
    ex:-$ kubectl exce -it gk --/bin/bash

 *) Check Resource Usage:-

    Verify resource usage (CPU, memory) of the misbehaving pod and its containers.
        $ kubectl top pods

 *) Review Application Configuration:-

    Ensure that the application's configuration is correct and matches the expected behavior.
         containers:
          - name: my-container
              image: my-image:tag
                env:
                 - name: ENV_VARIABLE
                    value: "value"

 *) Check Dependencies:-

     Verify if the misbehaving application depends on external services, and ensure they are accessible.
            containers:
            - name: my-container
               image: my-image:tag
                 ports:
               - containerPort: 80

 * Monitor Application Metrics:-

      If the application exposes metrics, use monitoring tools to check for abnormal behavior.
        containers:
         - name: my-container
            image: my-image:tag
            ports:
              - containerPort: 8080

 *) Inspect Environment Variables:-

     Double-check environment variables passed to the container.
       containers:
         - name: my-container
             image: my-image:tag
               env:
                 - name: DATABASE_URL
                   value: "mysql://user:password@hostname:port/database"

 *) Check for Ongoing Container Restarts:-

      If the pod is experiencing continuous restarts, investigate the reason for the restarts.
          $ kubectl describe pod <pod-name>
      ex:-$ kubectl describe pod gk

 *) Verify Network Policies:-

    Check if network policies are affecting communication within the pod or with external services.
         $ kubectl get networkpolicies

 *) Check for Resource Quotas and Limits:-

    Ensure that the pod is not hitting resource quotas (or) limits.
         $ kubectl get resourcequotas

 *) Examine Application Logs:-

    If the application logs are separate from container logs, review them for additional insights.
        $ kubectl exec -it <pod-name> -- cat /path/to/application/logs
    ex:-$ kubectl exec -it gk -- cat /webapps/prod/nginx/logs

 *) Inspect Cluster Events:-

    Look for any cluster events that might be affecting the pod.
       $ kubectl get events --sort-by=.metadata.creationTimestamp

 *) Check Pod Security Context:-

    Ensure that the security context settings in the pod specification are appropriate.
            securityContext:
               runAsUser: 1000

 *) Review Pod Labels and Annotations:-

     Check if the pod has the correct labels and annotations.
         metadata:
            labels:
              app: my-app
             annotations:
                description: "My Application"

 *) Check for Third-Party Dependencies:-

     If the application relies on external services or dependencies, ensure they are functioning correctly.
          containers:
           - name: my-container
             image: my-image:tag
             ports:
               - containerPort: 8080

API Server VM Shuts Down (or) Crashes:-

  1) Check Kubernetes API Server Logs:-

    *)  Use the $ kubectl logs command to access the logs of the API server pod in the kube-system namespace.
        $ kubectl logs -n kube-system <api-server-pod-name> -c kube-apiserver
    ex:-$ kubectl logs -n kube-system dev -c nginx
    *)  find any error messages (or) warnings that indicates the cause of the shutdown (or) crash.

  2) Check System Logs:-

   *) Review the system logs related to the Kubernetes API server to identify potential issues.
        $ journalctl -u kube-apiserver
   *) Examine the logs for any system-level errors or indications of problems.

  3) Resource Usage:-

   *) Ensure that the VM hosting the API server has sufficient resources.
   *) Check resource usage using "top (or) htop".
       $ kubectl top nodes
   *) Look here for high CPU (or) memory usage that might be contributing to the shutdown.

 4) Check Configuration:-

   *) Validate the Kubernetes API server configuration for any errors.
       $ kube-apiserver --config=<path-to-config-file> --dry-run
   ex:-$ kube-apiserver --config=dev/webapps/npm --dry-run
   *) Correct any configuration issues that are identified.

 5) Network Issues:-

   *) Verify the network configuration and check the connectivity issues.
       $ kubectl describe pod <api-server-pod-name> -n kube-system
   *) check for network-related errors (or) misconfigurations.

 6) Check Dependencies:-

  *) Confirm that all necessary dependencies "e.x., etcd are installed and running.
       $ systemctl status etcd
  *) Address any dependency issues that are identified.

 7) Update Software if required :-

   *) Ensure that your Kubernetes components are up-to-date.
      $ kubeadm upgrade plan
      $ kubeadm upgrade apply <desired-version>
   *) Follow the Kubernetes official  documentation for upgrading your cluster.

  8) Recreate the API Server Pod:-

   *) Deleting the API server pod will trigger Kubernetes to recreate it.
       $ kubectl delete pod <api-server-pod-name> -n kube-system
   ex:-$ kubectl delete pod kube-system dev -c nginx
   *) after delrting pod
       $ kubectl get pods 

  9) Check for Disk Space:-

    *) Insufficient disk space can cause issues. Verify available disk space.
         $ df -h
    *) Free up space or expand the disk if needed.

 10) Monitor for Ongoing Issues:-

  *) monitoring tools like Prometheus and Grafana to receive alerts.


Control Plane Service Shuts Down (or) Crashes:-

    control plane components of your Kubernetes cluster, such as the API server, controller manager, (or) etcd, 
    shut down or crash, it can significantly impact the stability and functionality of your cluster.

  1) Check Kubernetes Control Plane Component Logs:-

    *) Use the kubectl logs command to check the logs of the specific control plane component that is experiencing 
       issues. For ex, to check the logs of the API server
          $ kubectl logs -n kube-system <api-server-pod-name> -c kube-apiserver
      ex:-$ kubectl logs -n kube-system dev -c nginx
    *) Examine the logs for if error messages, warnings, (or) relevant information.

  2) Check System Logs:-

   *) Review the system logs on the nodes hosting the control plane components.
         $ journalctl -u kube-apiserver
   *) Look for system-level errors that might be related to the control plane service crashes.

  3) Resource Usage:-

    *) Ensure that the nodes hosting the control plane components have sufficient resources such as CPU, memory, 
       disk-space
        $ kubectl top nodes

  4) Check Configuration:-

   *) Validate the configuration files of the control plane components for errors.
       $ kube-apiserver --config=<path-to-api-server-config> --dry-run
   ex:-$ kube-apiserver --config=dev/webapps/nginx --dry-run
   *) Repeat this for other control plane components as needed.

 5) Network Issues:-

   *) Verify the network configuration and check for any issues affecting communication between control
      plane components.
        $ kubectl describe pod <control-plane-pod-name> -n kube-system
    ex:-$ kubectl describe pod dev -n kube-system
    *) Look for network-related errors or misconfigurations.

 6) Check Dependencies:-

   *) Confirm that dependencies like etcd are running and accessible.
        $ systemctl status etcd
   *) Resolve any issues related to dependencies.

 7) Update Software:-

   *) Ensure that your Kubernetes components are up-to-date.
      $ kubeadm upgrade plan
      $ kubeadm upgrade apply <desired-version>

 8) Restart Control Plane Components:-

   *) If the issue persists, try restarting the control plane components are affected.
       $ systemctl restart kube-apiserver
   *) just Monitor the component's status after the restart.

 9) Check for Disk Space if it is causing this issue:-

   *) Verify there is sufficient free disk space on the nodes.
      $ df -h
   *)  try to Address any disk space issues.

 10) Monitor for Ongoing Issues:-

   *) Set up monitoring tools like Prometheus and Grafana to receive alerts.

API Server Storage Lost:-

  1) Identify the Issue:-

     Determine the extent of the storage loss. Is it a complete loss, (or) is some data still recoverable?

  2) Restore from Backup:-

    If you have backups, use them to restore the API server data.
    ex:- Restoring etcd backup
    $ ETCDCTL_API=3 etcdctl --endpoints=https://<etcd-endpoint> --cacert=<aws-s3-ca-cert> --cert=
     <aws-s3-cert> --key=<dev/k8s/apiserver> snapshot restore <dev/aws/s3/apiserver>

 3) Recreate API Server Config:-

    If the API server configuration is lost, recreate it using the necessary parameters.
        $ kubeadm init --config=<path-to-config-file>
    ex:-$ kubeadm init --config=dev/aws/s3/k8s
    use the your original configuration file (or) use a new one if the old configuration is not recoverable.

 4) Recover etcd Data:-

    If the etcd data is lost, consider recovering it if possible.
    ex:- Recovering etcd data
     $ ETCDCTL_API=3 etcdctl --endpoints=https://<etcd-endpoint> --cacert=<aws/s3-ca-cert> --cert=<aws/s3-cert> 
       --key=<dev/k8s/apiserver/etcd> snapshot restore <dev/aws/s3/apiserver/etcd>

 5) Recreate API Server Pod:-

   If other measures fail, consider deleting and recreating the API server pod.
      $ kubectl delete pod -n kube-system <api-server-pod-name>
  ex:-$ kubectl delete pod -n kube-system dev/aws/s3/apiserver
      $ kubectl delete pod --all -n kube-system
    Kubernetes will automatically recreate the API server pod with the restored or recreated configuration.

 6) Rejoin Nodes:-

    If necessary, rejoin the worker nodes to the cluster.
    ex:-: Getting join command from the control plane
        $ kubeadm token create --print-join-command
    ex:-$ sudo kubeadm join 192.168.1.100:6443 --token dev.1234567890abcdef --discovery-token-ca-cert-hash
          sha256:1234567890abcdef1234567890abcdef1234567890abcdef1234567890abcdef

 7) Monitor and Validate:-

    Monitor the cluster to ensure that it returns to a stable state.
       $ kubectl get nodes
       $ kubectl get pods -n kube-system

 8) Prevent Future Data Loss:-

    Regularly back up your etcd data and other critical components to prevent future data loss.

 9) Review and Investigate:-

    *) Investigate the root cause of the storage loss to prevent it from happening again.
    *) Check system logs, disk health, and any events leading up to the storage loss.

