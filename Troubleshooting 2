Worker Node Shuts Down:-

 it can be due to various reasons such as hardware failures, network issues(or)resource exhaustion

  1) Identify the Issue:-

    Determine the reason for the worker node shutdown. Check system logs, hardware health, and network
    connectivity.

  2) Check Node Status:-

    Use $ kubectl to check the status of the node.
       $ kubectl get nodes
    Look for the status of the node in question. If it's not ready, investigate further.

 3) Check Node Logs:-

    View the node's logs to identify any issues.
       $ kubectl describe node <node-name>
   ex:-$ kubectl describe node nginx
   Look for events or conditions that might explain the shutdown.

 4) Check System Logs on Node:-

    Access system logs on the node to identify hardware (or) system-related issues.
        $ journalctl -xe
    Look for any critical errors (or) warnings.

 5) Check Resource Usage:-

    Use commands like top or htop to check resource usage on the node.
       $ ssh <node-worker1>
       $ top
    Identify resource-intensive processes (or) resource exhaustion.

 6) Check Disk Space:-

    Verify if there is sufficient disk space on the node.
       $ df -h
    Free up space or expand the disk if needed.

 7) Restart Node:-

   If the issue is not critical and you suspect it's a temporary problem, try restarting the node.
      $ sudo shutdown -r now

 8) Investigate Hardware Issues:-

    Check hardware health using system tools (or) vendor-specific utilities.
      $ sudo lshw -short
    Investigate any hardware failures reported.

 9) Check Network Connectivity:-

   Ensure there are no network issues affecting the node.
       $ ping <node-ip>
   ex:-$ ping 192.168.2.13
   Investigate and resolve any network-related problems.

 10) Recreate Node:-

   If the node remains problematic, consider draining and deleting it, allowing Kubernetes to recreate it.
       $ kubectl drain <node-name> --ignore-daemonsets
   ex:-$ kubectl drain worker-node1 --ignore-daemonsets
       $ kubectl delete node <node-name>
   ex:-$ kubectl delete node worker-node1
   New pods will be scheduled on other healthy nodes, and Kubernetes will recreate the problematic node.

 11) Monitor and Validate:-

   After taking corrective actions, monitor the cluster to ensure it stabilizes.
       $ kubectl get nodes
       $ kubectl get pods --all-namespaces

 12) Prevent Future Issues:-

   *) Implement preventive measures, such as monitoring, to catch issues early.
   *) Regularly review node and system health.

 13) Review Incident:-

    Conduct a post-incident review to understand the root cause and prevent recurrence.

Kubelet Malfunction:-

    A malfunctioning kubelet in your Kubernetes cluster can lead to various issues, such as pod scheduling problems,
    container runtime errors, (or) node instability.

  1) Identify the Issue:-

    Check logs and events to understand the nature of the kubelet malfunction.
         $ journalctl -u kubelet

  2) Check Node Status:-

    Use kubectl to check the status of the node associated with the malfunctioning kubelet.
         $ kubectl get nodes

 3) Restart the Kubelet:-

   Restarting the kubelet might resolve transient issues.
        $ sudo systemctl restart kubelet

 4) Check Kubelet Logs:-

   Inspect the kubelet logs for any errors or warning messages.
       $ journalctl -u kubelet

 5) Check Kubelet Configuration:-

   Verify the kubelet configuration for correctness.
       $ cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

 6) Recreate Kubelet Configuration:-

    If the kubelet configuration is corrupted, recreate it.
      $ kubeadm config print init-defaults > kubeadm.conf

 7) Update Kubelet:-

   Ensure that your Kubernetes components are up-to-date.
     $ kubeadm upgrade plan
     $ kubeadm upgrade apply <desired-version>

 8) Check Network Configuration:-

   Verify the network configuration on the node.
     $ cat /etc/cni/net.d/*

 9) Check Container Runtime:-

   Inspect the container runtime logs for any issues.
     $ journalctl -u docker

 10) Restart Docker (or Containerd, CRI-O, etc.):-

   Restart the container runtime if needed.
      $ sudo systemctl restart docker

 11) Node Re-registration:-

   Re-register the node to the cluster if necessary.
      $ kubeadm token create --print-join-command
  ex:-$ sudo kubeadm join 192.168.1.100:6443 --token abcdef.1234567890abcdef --discovery-token-ca-
        cert-hash sha256:1234567890abcdef1234567890abcdef1234567890abcdef1234567890abcdef

 12) Check Resource Usage:-

    Use commands like top or htop to check resource usage.
       $ ssh <node-name>
       $ top
   ex:-$ ssh worker-node1

 13) Reinstall Kubelet:-

    If the issue persists, consider reinstalling the kubelet.
     $ sudo kubeadm reset
     $ sudo apt-get remove --purge kubelet kubeadm kubectl
     $ sudo apt-get install kubelet kubeadm kubectl
     $ sudo kubeadm init --ignore-preflight-errors=all

 14) Monitor and Validate:-

    Monitor the kubelet logs and node status for any improvements.
     $ journalctl -u kubelet
     $ kubectl get nodes

 15) Prevent Future Issues:-

   *) Implement preventive measures, such as monitoring and regular maintenance.
   *) Keep your system and Kubernetes components up-to-date.

Unplanned Network Partitioning Disconnecting Some Nodes from the Master:-

   it can lead to various issues such as node instability, pod scheduling problems, or degraded cluster
   performance

 1) Identify the Network Partition:-

   *) Determine which nodes are disconnected from the master.
        $ kubectl get nodes
   *) Identify nodes in a "NotReady" state.

 2) Check Network Connectivity:-

    Verify network connectivity between the master and disconnected nodes.
        $ ping <disconnected-node-ip>
    ex:-$ ping 192.168.2.19

 3) Check Node Logs:-

    Inspect the logs of the disconnected nodes for any network-related issues.
        $ kubectl logs <disconnected-node-name>
    ex:-$ kubectl logs worker-node1

 4) Check Master Node Logs:-

    Inspect the logs on the master node for any network-related errors.
        $ journalctl -u kubelet

 5) Check iptables Rules:-

    Review iptables rules on both master and nodes to ensure proper network communication.
        $ iptables -L -n

 6) Restart Affected Nodes:-

     Restart nodes that are experiencing network issues.
        $ sudo systemctl restart kubelet

 7) Check Kubelet Configuration:-

    Verify the kubelet configuration for correctness.
        $ cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

 8) Rejoin Nodes:-

    If needed, rejoin disconnected nodes to the cluster.
        $ kubeadm token create --print-join-command
    ex:-$ sudo kubeadm join 192.168.1.100:6443 --token abcdef.1234567890abcdef --discovery-token-ca-
        cert-hash sha256:1234567890abcdef1234567890abcdef1234567890abcdef1234567890abcdef

 9) Check etcd Health:-
 
     Check the health of the etcd cluster (if used).
       $ ETCDCTL_API=3 etcdctl --endpoints=https://<etcd-endpoint> --cacert=<path-to-ca-cert> --cert=
         <path-to-cert> --key=<path-to-key> endpoint health
   ex:-$ ETCDCTL_API=3 etcdctl --endpoints=https://<etcd-endpoint> --cacert=<aws/s3/ca/cert> --cert=
         <aws/s3/cert> --key-<dev/aws/s3> endpoint health

 10) Restart Master Components:-

    If the master node is affected, consider restarting the necessary control plane components.
       $ sudo systemctl restart kube-apiserver

 11) Review Network Configuration:-

    Verify the network configuration on both master and nodes.
       $ cat /etc/cni/net.d/*

 12)  Recreate iptables Rules:-

    If needed, recreate iptables rules on both master and nodes.
       $ iptables-restore < /etc/sysconfig/iptables

 13) Monitor and Validate:-

    Monitor the nodes and master for any improvements.
      $ kubectl get nodes
      $ kubectl get pods --all-namespaces

 14) Prevent Future Network Issues:-

    *) Implement preventive measures such as network monitoring and redundant network paths.
    *) Regularly review network configurations.

Human Error by Cluster Operator:-

  1) Identify the Nature of the Error:-

     Understand the specific nature of the human error. This could range from misconfiguration, accidental
     deletion of resources, (or) incorrect updates.

  2) Check Cluster Status:-

    Assess the current state of the cluster to understand the impact of the human error.
       $ kubectl get nodes
       $ kubectl get pods --all-namespaces

  3) Review Logs:-

     Inspect relevant logs to gather more details about the error.
       $ kubectl logs <pod-name> -n <namespace>
   ex:-$ kubectl logs gk -n prod

  4) Identify and Confirm the Error:-

    Pinpoint the specific mistake made and confirm its impact.
    ex:- If a critical pod was deleted accidentally,
       $ kubectl get pods -n <namespace>
   ex:-$ kubectl get pods -n prod

 5) Recreate (or) Restore Resources:-

    Recreate (or) restore the affected resources based on the nature of the error.
    ex: If a deployment was accidentally deleted,
       $ kubectl apply -f <path-to-deployment.yaml>
   ex:-$ kubectl apply -f dev/aws/s3/webapps-deployment.yaml

 6) Check Configurations:-

   If the error is related to misconfigurations, review and correct the configurations.
   ex:- If a misconfigured Ingress caused issues,
      $ kubectl edit ingress <ingress-name> -n <namespace>
  ex:-$ kubectl edit ingress gk -n dev

 7) Implement Rollback:-

   If an erroneous update was applied, consider rolling back to the previous state.
   ex:- Rollback a deployment to the previous revision,
      $ kubectl rollout undo deployment <deployment-name> -n <namespace>
  ex:-$ kubectl rollout undo deployment roll_deployment -n dev

  8) Check Dependencies:-

     Confirm that dependencies are intact and operational.
     ex:- If a ConfigMap used by a pod was deleted
       $ kubectl get configmap -n <namespace>
   ex:-$ kubectl get configmap -n dev

 9) Communicate and Document:-

    If multiple people are involved, communicate the issue, and document the steps taken for resolution.

Unavailable nodes:-

 it can lead to service disruptions and impact the stability of your applications. 

  1) Identify Unavailable Nodes:-

   *) Use kubectl to check the status of nodes in your cluster.
       $ kubectl get nodes
   *) Identify nodes in a NotReady (or) Unknown state.

 2) Check Node Conditions:-

  *) Investigate the node conditions to understand why nodes are not ready.
       $ kubectl describe node <node-name>
   ex:-$ kubectl describe node gk
  *) Look for any conditions that might explain the unavailability.

 3) Check Node Logs:-

  *) Examine the logs of the affected nodes for any error messages.
       $ kubectl logs <node-name>
   ex:-$ kubectl logs gk
  *) Check for network-related issues, disk errors, (or) other problems.

 4) Restart kubelet Service:-

  *) Restart the kubelet service on the affected node.
       $ sudo systemctl restart kubelet
  *) Monitor the node status after the restart.

 5) Check Resource Usage:-

  *)  Use commands like top or htop to check resource usage on the nodes.
       $ ssh <node-name>
       $ top
   ex:-$ ssh gk
  *) Identify resource-intensive processes (or) resource exhaustion.

 6) Check Disk Space:-

  *) Verify if there is sufficient disk space on the nodes.
      $ df -h
  *) Free up space or expand the disk if needed.

 7) Check Network Connectivity:-

  *) Ensure there are no network issues affecting the nodes.
      $ ping <node-ip>
  ex:-$ ping 192.168.1.23
  *) Investigate and resolve any network-related problems.

 8) Check for System Updates:-

    Ensure that the node's operating system and software are up-to-date.
      $ sudo apt update && sudo apt upgrade -y

 9) Node Re-registration:-

    *) If necessary, rejoin the affected nodes to the cluster.
       $ kubeadm token create --print-join-command
   ex:-$ sudo kubeadm join 192.168.1.100:6443 --token abcdef.1234567890abcdef --discovery-token-ca-
        cert-hash sha256:1234567890abcdef1234567890abcdef1234567890abcdef1234567890abcdef
    *) Execute the join command on the affected nodes.

 10) Check kube-proxy:-

   *) Restart kube-proxy to ensure proper networking functionality.
       $ kubectl rollout restart daemonset kube-proxy -n kube-system

 11) Check Cluster Networking:-

    Validate the health of the cluster networking components.
      $ kubectl get pods -n kube-system

 12) Recreate Nodes:-

   *) If the issue persists, consider draining and deleting the problematic nodes, allowing Kubernetes to
      re-create them.
      $ kubectl drain <node-name> --ignore-daemonsets
  ex:-$ kubectl drain gk --ignore-daemonsets
      $ kubectl delete node <node-name>
  ex:-$ kubectl delete node gk
   *) New pods will be scheduled on other healthy nodes, and Kubernetes will recreate the problematic nodes.

Noisy neighbors:-

    it typically refers to situations where certain pods (or) containers on the same node are consuming an excessive
    amount of resources, negatively impacting the performance of other pods on the same node.

  1) Identify Resource-Intensive Pods:-

     Use resource monitoring tools to identify pods (or) containers consuming excessive CPU (or) memory.
         $ kubectl top pods --all-namespaces

  2) Check Resource Requests and Limits:-

     Review the resource requests and limits specified in the pod manifests.
      resources:
         requests:
            memory: "64Mi"
            cpu: "250m"
          limits:
              memory: "128Mi"
              cpu: "500m"

  3) Modify Resource Specifications:-

    Adjust resource requests and limits to better match the actual needs of the pods.
       resources:
         requests:
           memory: "128Mi"
           cpu: "250m"
         limits:
            memory: "256Mi"
            cpu: "500m"

 4) Isolate Pods on Nodes:-

   Use node affinity rules to schedule pods on different nodes.
    affinity:
     nodeAffinity:
       requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
           - matchExpressions:
              - key: dedicated
                 operator: In
                  values:
                - <node-label>

 5) Implement Pod Priority:-

   Use pod priority and preemption to ensure higher-priority pods get preferential treatment.
       priorityClassName: high-priority

 6) Implement QoS Classes:-

   Utilize Quality of Service (QoS) classes to categorize pods based on resource requirements.
           resources:
             requests:
                 memory: "64Mi"
                 cpu: "250m"

 7) Use Pod Disruption Budgets:-

     Implement PodDisruptionBudgets to control the number of pods that can be disrupted simultaneously.
          apiVersion: policy/v1beta1
          kind: PodDisruptionBudget
          metadata:
              name: gk-pdb
             spec:
                maxUnavailable: 1
                selector:
                matchLabels:
                   app: depolyment

 8) Utilize Horizontal Pod Autoscaling:-

    Automatically adjust the number of pod replicas based on resource usage.
     apiVersion: autoscaling/v1
     kind: HorizontalPodAutoscaler
      metadata:
        name: deployment-hpa
     spec:
        scaleTargetRef:
             apiVersion: apps/v1
             kind: Deployment
             name: example-deployment
          minReplicas: 1
          maxReplicas: 10
             targetCPUUtilizationPercentage: 50

 9) Use Resource Quotas:-

    Implement Resource Quotas to limit the total amount of resources a namespace can consume.
          apiVersion: v1
          kind: ResourceQuota
          metadata:
              name: deployment-quota
          spec:
              hard:
                 pods: "10"
                  requests.cpu: "1"
                  requests.memory: "1Gi"
                  limits.cpu: "2"
                  limits.memory: "2Gi"
